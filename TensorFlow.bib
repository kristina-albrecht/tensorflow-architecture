Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{TensorFlowArchitecture2018,
author = {TensorFlow},
title = {{TensorFlow Architecture}},
url = {https://www.tensorflow.org/extend/architecture},
year = {2018}
}
@article{Forschungsseminar2018,
file = {:media/kristina/Daten/KRISTINA/Education/DHBW/tensorflow-architecture/img/Taeschner_Ausarbeitung.pdf:pdf},
title = {{Forschungsseminar Deep Learning Abteilung Datenbanken WS 2017/2018}},
year = {2018}
}
@misc{Graph2018,
author = {TensorFlow},
title = {{TensorFlow: Graphs and Sessions}},
url = {https://www.tensorflow.org/programmers_guide/graphs},
year = {2018}
}
@article{Abadiand2015,
abstract = {Nature Neuroscience 16, 486 (2013). doi:10.1038/nn.3331},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Schapiro, Anna C. and Rogers, Timothy T. and Cordova, Natalia I. and Turk-Browne, Nicholas B. and Botvinick, Matthew M.},
doi = {10.1038/nn.3331},
eprint = {arXiv:1408.1149},
isbn = {1546-1726 (Electronic)\n1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {4},
pages = {486--492},
pmid = {23416451},
title = {{Neural representations of events arise from temporal community structure}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
volume = {16},
year = {2013}
}
@article{LargeScale2012,
abstract = {Recentwork in unsupervised feature learning and deep learning has shown that be- ing able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network train- ing. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k cate- gories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition ser- vice. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1223--1231},
pmid = {43479959},
title = {{Large scale distributed deep networks}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2012}
}
@misc{DelftUniversity2017,
abstract = {TensorFlow™ is an open source software library developed by the Google Brain team for the purpose of conducting machine learning and deep neural networks research. The library performs numerical computation by using data flow graphs, where the nodes in the graph represent mathematical operations and the graph edges represent the multidimensional data arrays (tensors) which communicate between the nodes. The API has been used in the fields of medicine, translation services, and the analysis of financial markets. This chapter first describes TensorFlow™ by its features and stakeholders, secondly the architecture is analyzed by means of the context, development, and deployment view, and finally a conclusion is provided.},
author = {Chan-Zheng, Carmen and Verdiesen, Ilse and Carvajal-Godinez, Johan and {Sailesh Mani}, Pranav},
title = {{TensorFlow™ - Open Source Library for Machine Learning Applications}},
url = {https://delftswa.gitbooks.io/desosa2016/content/tensorflow/chapter.html},
year = {2017}
}
@misc{GitHub2018,
author = {TensorFlow},
title = {{TensorFlow GitHub}},
url = {https://github.com/tensorflow/tensorflow/tree/master/tensorflow},
year = {2018}
}
