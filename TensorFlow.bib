@article{Dean2012,
abstract = {Recentwork in unsupervised feature learning and deep learning has shown that be- ing able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network train- ing. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k cate- gories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition ser- vice. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1223--1231},
pmid = {43479959},
title = {{Large scale distributed deep networks}},
year = {2012}
}
@misc{TensorFlow2018,
author = {TensorFlow},
title = {{TensorFlow Architecture}},
url = {https://www.tensorflow.org/extend/architecture},
year = {2018}
}
@article{Hallo2018,
author = {Hallo},
file = {:media/kristina/Daten/KRISTINA/Education/DHBW/tensorflow-architecture/img/Taeschner_Ausarbeitung.pdf:pdf},
title = {{Forschungsseminar Deep Learning Abteilung Datenbanken WS 2017/2018}},
year = {2018}
}
@article{Abadiand2015,
abstract = {Nature Neuroscience 16, 486 (2013). doi:10.1038/nn.3331},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Schapiro, Anna C. and Rogers, Timothy T. and Cordova, Natalia I. and Turk-Browne, Nicholas B. and Botvinick, Matthew M.},
doi = {10.1038/nn.3331},
eprint = {arXiv:1408.1149},
isbn = {1546-1726 (Electronic)\n1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {4},
pages = {486--492},
pmid = {23416451},
title = {{Neural representations of events arise from temporal community structure}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
volume = {16},
year = {2013}
}
